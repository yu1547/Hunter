import ollama
import json
import os
import logging
from dotenv import load_dotenv
from llm_utils import (
    get_self_check_prompt,
    parse_paragraph,
    cache_embeddings,
    calc_similar_vectors,
    classify_question_type,
    build_instance_adaptive_prompt,
    summarize_conversation,
)
from route_utils import detect_route_intent, generate_route_from_start

load_dotenv()

# è¨­å®š logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ– RAG æ‰€éœ€çš„è³‡æº
doc = "rule.txt"
paragraphs = None
embeddings = None

def get_embeddings():
    global paragraphs, embeddings
    if paragraphs is None or embeddings is None:
        paragraphs = parse_paragraph(doc)
        embeddings = cache_embeddings(doc, paragraphs)
    return paragraphs, embeddings

api_key = os.getenv("GOOGLE_API_KEY")

def handle_chat_request(prompt, conversation_history, enable_self_check=True):
    """è™•ç†èŠå¤©è«‹æ±‚ï¼ŒåŒ…å« RAG å’Œè‡ªæˆ‘æª¢æŸ¥é‚è¼¯"""
    try:
        paragraphs, embeddings = get_embeddings()
        question_type = classify_question_type(prompt)
        logger.info(f"ğŸ§  å•é¡Œåˆ†é¡çµæœï¼š{question_type}")

        try:
            prompt_embedding = ollama.embeddings(model="ycchen/breeze-7b-instruct-v1_0", prompt=prompt)["embedding"]
        except Exception as e:
            logger.error(f"âŒ LLM embeddings ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"LLM embeddings ç™¼ç”ŸéŒ¯èª¤: {e}"

        try:
            similar_vectors = calc_similar_vectors(prompt_embedding, embeddings)[:3]
            valid_vectors = [v for v in similar_vectors if v[0] < len(paragraphs)]

            memory_summary = None
            if len(conversation_history) >= 2:
                memory_summary = summarize_conversation(conversation_history[-4:])

            system_prompt = build_instance_adaptive_prompt(paragraphs, valid_vectors, question_type, memory_summary)
            messages = [{"role": "system", "content": system_prompt}] + conversation_history

            if enable_self_check:
                max_attempts = 3
                attempt = 1
                final_response = None
                error_feedback = None
                while attempt <= max_attempts:
                    if error_feedback:
                        retry_hint = f"ä¸Šæ¬¡å›è¦†è¢«åˆ¤å®šç‚ºä¸åˆæ ¼ï¼ŒåŸå› æ˜¯ï¼š{error_feedback.strip()}ã€‚\nè«‹éµå®ˆè¦å‰‡ï¼Œè«‹å‹¿å†çŠ¯ä¸‹åŒæ¨£çš„éŒ¯èª¤ã€‚\n\n"
                    else:
                        retry_hint = ""
                    modified_system_prompt = retry_hint + system_prompt
                    current_messages = [{"role": "system", "content": modified_system_prompt}] + conversation_history
                    try:
                        response = ollama.chat(model="ycchen/breeze-7b-instruct-v1_0", messages=current_messages)["message"]["content"]
                    except Exception as e:
                        logger.error(f"âŒ LLM chat ç™¼ç”ŸéŒ¯èª¤: {e}")
                        return f"LLM chat ç™¼ç”ŸéŒ¯èª¤: {e}"

                    logger.info(f"\nğŸ—¨ï¸ å›è¦†å…§å®¹ï¼ˆç¬¬ {attempt} æ¬¡å˜—è©¦ï¼‰:\n{response}\n")

                    # å‘¼å« get_self_check_prompt
                    check_response_prompt = get_self_check_prompt(question_type, response)
                    try:
                        audit_result = ollama.chat(
                            model="ycchen/breeze-7b-instruct-v1_0",
                            messages=[{"role": "user", "content": check_response_prompt}]
                        )["message"]["content"]
                    except Exception as e:
                        logger.error(f"âŒ LLM è‡ªæˆ‘æª¢æŸ¥ chat ç™¼ç”ŸéŒ¯èª¤: {e}")
                        return f"LLM è‡ªæˆ‘æª¢æŸ¥ chat ç™¼ç”ŸéŒ¯èª¤: {e}"

                    logger.info(f"ğŸ§ª è‡ªæˆ‘æª¢æŸ¥çµæœï¼š{audit_result.strip()}\n")

                    if "ä¸åˆæ ¼" in audit_result:
                        logger.warning("âŒ ä¸åˆæ ¼ï¼Œé‡æ–°ç”Ÿæˆæ–°çš„å›ç­”...")
                        error_feedback = audit_result.replace("ä¸åˆæ ¼ï¼š", "").strip()
                        attempt += 1
                    else:
                        final_response = response
                        break

                if final_response is None:
                    logger.warning("[âš ï¸ æœ€å¤šé‡è©¦æ¬¡æ•¸å·²é”ï¼Œå›ç­”æˆ‘ä¸æ¸…æ¥šéŠæˆ²ä»¥å¤–çš„å…§å®¹]")
                    final_response = "æˆ‘ä¸æ¸…æ¥šéŠæˆ²ä»¥å¤–çš„å…§å®¹"
                return final_response
            else:
                try:
                    response = ollama.chat(model="ycchen/breeze-7b-instruct-v1_0", messages=messages)["message"]["content"]
                except Exception as e:
                    logger.error(f"âŒ LLM chat ç™¼ç”ŸéŒ¯èª¤: {e}")
                    return f"LLM chat ç™¼ç”ŸéŒ¯èª¤: {e}"
                logger.info(f"\nğŸ—¨ï¸ å›è¦†å…§å®¹ï¼š\n{response}\n")
                return response
        except Exception as e:
            logger.error(f"âŒ handle_chat_request å…§éƒ¨æµç¨‹éŒ¯èª¤: {e}")
            return f"handle_chat_request å…§éƒ¨æµç¨‹éŒ¯èª¤: {e}"
    except Exception as e:
        logger.error(f"âŒ handle_chat_request ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        return f"handle_chat_request ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}"

def handle_route_request(user_location, candidate_landmarks, enable_self_check=True,api_key=None):
    """è™•ç†è·¯ç·šè¦åŠƒè«‹æ±‚"""
    try:
        start_lat = user_location.get("latitude")
        start_lon = user_location.get("longitude")

        if not all([start_lat, start_lon, api_key, candidate_landmarks]):
            raise ValueError("ç¼ºå°‘ç”Ÿæˆè·¯ç·šæ‰€éœ€çš„åƒæ•¸ï¼ˆç¶“ç·¯åº¦ã€APIé‡‘é‘°æˆ–å€™é¸åœ°æ¨™ï¼‰ã€‚")

        try:
            response_data = generate_route_from_start(start_lat, start_lon, candidate_landmarks, api_key=api_key)
            logger.info(f"\nğŸ—ºï¸ è·¯ç·šè¦åŠƒçµæœï¼š\n{json.dumps(response_data, ensure_ascii=False, indent=2)}\n")
            return response_data
        except Exception as e:
            logger.error(f"\nâŒ è·¯ç·šè¦åŠƒæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\n")
            return {"error": f"è·¯ç·šè¦åŠƒæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"}
    except Exception as e:
        logger.error(f"âŒ handle_route_request ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        return {"error": f"handle_route_request ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}"}
